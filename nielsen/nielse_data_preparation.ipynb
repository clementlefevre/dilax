{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import date\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import PandasHelper as pdh\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "MAIN_FILE = DATA_PATH+'nielsen.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_name</th>\n",
       "      <th>first_timeframe</th>\n",
       "      <th>dwell_time_s</th>\n",
       "      <th>device_id</th>\n",
       "      <th>visitor</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BF Karlsruhe Kaiserstr (1122)</td>\n",
       "      <td>2014-12-31 23:00:30</td>\n",
       "      <td>15</td>\n",
       "      <td>bd5d8c2890622782d681c82f4dd84db4</td>\n",
       "      <td>True</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BF Karlsruhe Kaiserstr (1122)</td>\n",
       "      <td>2014-12-31 23:00:40</td>\n",
       "      <td>1080</td>\n",
       "      <td>428fa91d6d741e1466b4bcd917dff4c2</td>\n",
       "      <td>True</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       site_name     first_timeframe  dwell_time_s                         device_id visitor        date\n",
       "0  BF Karlsruhe Kaiserstr (1122) 2014-12-31 23:00:30            15  bd5d8c2890622782d681c82f4dd84db4    True  2014-12-31\n",
       "1  BF Karlsruhe Kaiserstr (1122) 2014-12-31 23:00:40          1080  428fa91d6d741e1466b4bcd917dff4c2    True  2014-12-31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(MAIN_FILE, parse_dates=['first_timeframe'],nrows=1000)\n",
    "df['date']=df['first_timeframe'].dt.date\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Create indexes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk_and_retrieve_indexes():\n",
    "    chunks = pd.read_csv(MAIN_FILE,chunksize=1000000)\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        print chunk.shape\n",
    "        devices_ix = pd.DataFrame(chunk.device_id.unique())\n",
    "        sites_ix = pd.DataFrame(chunk.site_name.unique())\n",
    "        devices_ix.to_csv(DATA_PATH+\"indexes/devices/devices_ix_\"+str(i)+\".csv\")\n",
    "        sites_ix.to_csv(DATA_PATH+\"indexes/sites/sites_ix_.csv\"+str(i)+\".csv\")\n",
    "\n",
    "def concatenate_df(df,path):\n",
    "    df1 = pd.read_csv(path,index_col=0)\n",
    "   \n",
    "    return pd.concat([df,df1]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def concatenate_index_files():\n",
    "    index_devices_files = pdh.get_files(DATA_PATH+\"indexes/devices/\")\n",
    "    index_sites_files = pdh.get_files(DATA_PATH+\"indexes/sites/\")\n",
    "    \n",
    "    df_devices_ix = pd.DataFrame()\n",
    "    df_sites_ix = pd.DataFrame()\n",
    "    \n",
    "    for index_device_file in index_devices_files:\n",
    "        print index_device_file\n",
    "        df_devices_ix = concatenate_df(df_devices_ix,DATA_PATH+\"indexes/devices/\"+index_device_file)\n",
    "    df_devices_ix.columns=['id','device_mac']\n",
    "    df_devices_ix.to_csv(DATA_PATH+\"indexes/devices_ix.csv\")\n",
    "    \n",
    "    for index_sites_file in index_sites_files:\n",
    "        df_sites_ix = concatenate_df(df_sites_ix,DATA_PATH+\"indexes/sites/\"+index_sites_file)\n",
    "    df_sites_ix.columns=['id','site_name']\n",
    "    df_sites_ix.to_csv(DATA_PATH+\"indexes/sites_ix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-replace devices and sites name with ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_agg_with_device_and_sites_index():\n",
    "    chunks=pd.read_csv(DATA_PATH+\"nielsen.csv\",chunksize=1000000)\n",
    "    df_devices_idx = pd.read_csv(DATA_PATH+\"indexes/devices_ix.csv\")\n",
    "    df_sites_index = pd.read_csv(DATA_PATH+\"indexes/sites_ix.csv\")\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        print chunk.shape, i\n",
    "        chunk = pd.merge(chunk, df_devices_idx, left_on='device_id', right_on='device_mac',suffixes=('_agg', '_devicesidx'))\n",
    "        chunk = chunk[['id','site_name','first_timeframe']]\n",
    "        chunk = pd.merge(chunk, df_sites_index, on='site_name', suffixes=('_devices','_sites'))\n",
    "        chunk=chunk[['id_devices', 'id_sites']]\n",
    "        chunk=chunk.astype('int32')\n",
    "        df = pd.concat([df,chunk], axis=0, ignore_index=True)\n",
    "    print \"merge over,starting saving to disk...\"\n",
    "    df.to_csv(DATA_PATH+'/nielsen_indexes.csv')\n",
    "    print \"saved to disk OK.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Aggregate per devices and site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate(chunk):\n",
    "    \n",
    "    chunk['count']=1\n",
    "    groupy = chunk.groupby(['id_devices','id_sites']).count()\n",
    "    groupy = groupy.reset_index()\n",
    "    \n",
    "    groupy =groupy.groupby('id_devices')['id_sites'].apply(lambda x: x.tolist())\n",
    "    return pd.DataFrame(groupy)\n",
    "\n",
    "def aggregate_per_devices_and_sites():\n",
    "    chunks = pd.read_csv(DATA_PATH+'/nielsen_indexes.csv',chunksize=40000000,index_col=0)\n",
    "    df =pd.DataFrame()\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        df = pd.concat([df,aggregate(chunk)], axis=1, ignore_index=False)\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        print df.shape,i\n",
    "    df.to_csv(DATA_PATH+'/nielsen_indexes_pivot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4- Clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combine_sites_id(df):\n",
    "    cols = df.columns.tolist()\n",
    "    df = df.replace('0', 0)\n",
    "    df =df.replace(0,\"\")\n",
    "    df = df.fillna(\"\")\n",
    "    df['sites_id']= df[cols].astype(str).sum(axis=1)\n",
    "    df['sites_id_array']= df['sites_id'].apply(pdh.string_to_np_array)\n",
    "    df['sites_count']= df['sites_id_array'].apply(lambda x: x.size)\n",
    "    df = df[['sites_id_array','sites_count']]\n",
    "    df.reset_index()\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def clean_data():\n",
    "    chunks = pd.read_csv(DATA_PATH+'nielsen_indexes_pivot.csv',chunksize=1000000,index_col=0)\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        df = pd.concat([df,combine_sites_id(chunk)],axis=0)\n",
    "        print df.shape,i\n",
    "    print \"Combining done, saving to file...\"\n",
    "    df.to_csv(DATA_PATH+\"nielsen_indexes_sites_per_devices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5- Filter data on devices that appeared on more than one site\n",
    "once a list of devices/sites/number of sites detected has been created, we can easily filter on devices with multiple site appearances to reduce the dataset of 67%, to a dataset of around 6 millions devices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_mask_on_devices_that_appears_on_multiple_sites():\n",
    "    df = pd.read_csv(DATA_PATH+\"/nielsen_indexes_sites_per_devices.csv\", index_col=0)\n",
    "    mask = pd.DataFrame(df[df.sites_count>1].index.values)\n",
    "    mask.columns=['device_id']\n",
    "    \n",
    "    df_devices_indexes = pd.read_csv(DATA_PATH+\"indexes/devices_ix.csv\")\n",
    "    df = pd.merge(mask,df_devices_indexes,left_on=\"device_id\",right_on=\"id\")\n",
    "    df=df[['device_id','device_mac']]\n",
    "    df.to_csv(DATA_PATH+\"indexes/devices_ix_multiple_sites.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Merge devices with multiple sites with raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_data(chunk,mask,sites_id):\n",
    "    chunk = chunk.reset_index()\n",
    "    chunk['date']=chunk['first_timeframe'].dt.date\n",
    "\n",
    "    df1 = pd.merge(chunk,mask,left_on='device_id',right_on='device_mac')[['device_id_y','site_name','date']]\n",
    "    df1 = pd.merge(df1,sites_id,on='site_name')[['device_id_y','id','date']]\n",
    "    df1.columns = ['device_id','site_id','date']\n",
    "    return df1\n",
    "\n",
    "def reduce_data_on_mask():\n",
    "    devices_id_chunks = pd.read_csv(DATA_PATH+\"indexes/devices_ix_multiple_sites.csv\", chunksize=500000,index_col=0)\n",
    "    sites_id = pd.read_csv(DATA_PATH+\"indexes/sites_ix.csv\",index_col=0)\n",
    "    sites_id = sites_id.reset_index()  \n",
    "    \n",
    "    for i,devices_id_chunk in enumerate(devices_id_chunks):\n",
    "        data_chunks = pd.read_csv(DATA_PATH+'nielsen.csv',parse_dates=['first_timeframe'],chunksize=1000000,index_col=0)\n",
    "        print \"Merging \"+str(i)+\" Starts.------------------------------\"\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j,data_chunk in enumerate(data_chunks):\n",
    "            df = pd.concat([df,merge_data(data_chunk,devices_id_chunk,sites_id)],axis=0)\n",
    "            print df.shape,j\n",
    "        print \"Finished merging, saving to file...\"\n",
    "        df.to_csv(DATA_PATH+\"nielsen_data_multiple_sites_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-Group filtered raw data per device-date-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_count(file_name):\n",
    "    df = pd.read_csv(DATA_PATH+\"data_multiples_sites_only_with_indexes/\"+file_name,index_col=0)\n",
    "    result = df.groupby(['device_id','date'])['site_id'].apply(lambda x: (np.unique(x.values).size))\n",
    "    final = pd.DataFrame(result)\n",
    "    final.columns=[['count_sites']]\n",
    "    file_ix = re.findall(r'\\d+',file_name)[0]\n",
    "    final.to_csv(DATA_PATH+\"groupby_device_date_site/groupby_device_date_site\"+str(file_ix)+\".csv\")\n",
    "    \n",
    "def groupby_device_date_site():\n",
    "    files = pdh.get_files(DATA_PATH+\"data_multiples_sites_only_with_indexes/\")\n",
    "    \n",
    "    for filo in files:\n",
    "        print \"grouping :\"+ filo\n",
    "        final_count(filo)\n",
    "    \n",
    "    groupby_files = pdh.get_files(DATA_PATH+\"groupby_device_date_site/\")\n",
    "    df = pd.DataFrame()\n",
    "    for groupbyfilo in groupby_files:\n",
    "        df1 = pd.read_csv(DATA_PATH+\"groupby_device_date_site/\"+groupbyfilo,index_col=0)\n",
    "        df = pd.concat([df,df1],axis=0)\n",
    "    df.to_csv(DATA_PATH+\"groupby_device_date_site_final.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Some statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
